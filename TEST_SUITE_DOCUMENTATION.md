# Test Suite Documentation
**Last Updated**: November 2, 2025
**Status**: All tests passing (100% standalone, 98.8% pytest)

---

## Table of Contents
1. [Test Organization](#test-organization)
2. [Prerequisites & Environment Setup](#prerequisites--environment-setup)
3. [Execution Order](#execution-order)
4. [Test Categories](#test-categories)
5. [Critical Discoveries & Lessons](#critical-discoveries--lessons)
6. [Model Management](#model-management)
7. [Troubleshooting](#troubleshooting)

---

## Test Organization

### Directory Structure
```
lmstudio-bridge-enhanced/
‚îú‚îÄ‚îÄ tests/                          # Pytest suite (172 tests)
‚îÇ   ‚îú‚îÄ‚îÄ fixtures/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ model_management.py     # Model loading fixtures (NEW)
‚îÇ   ‚îú‚îÄ‚îÄ test_constants.py
‚îÇ   ‚îú‚îÄ‚îÄ test_e2e_multi_model.py
‚îÇ   ‚îú‚îÄ‚îÄ test_error_handling.py
‚îÇ   ‚îú‚îÄ‚îÄ test_exceptions.py
‚îÇ   ‚îú‚îÄ‚îÄ test_failure_scenarios.py
‚îÇ   ‚îú‚îÄ‚îÄ test_mcp_health_check_demo.py
‚îÇ   ‚îú‚îÄ‚îÄ test_model_validator.py
‚îÇ   ‚îú‚îÄ‚îÄ test_multi_model_integration.py
‚îÇ   ‚îú‚îÄ‚îÄ test_performance_benchmarks.py
‚îÇ   ‚îî‚îÄ‚îÄ test_validation_security.py
‚îÇ
‚îî‚îÄ‚îÄ test_*.py                       # Standalone tests (28 scripts)
    ‚îú‚îÄ‚îÄ test_all_apis_comprehensive.py                  # API integration
    ‚îú‚îÄ‚îÄ test_llmclient_error_handling_integration.py    # Error handling (RENAMED)
    ‚îú‚îÄ‚îÄ test_mcp_tool_model_parameter_support.py        # MCP tools (RENAMED)
    ‚îú‚îÄ‚îÄ test_reasoning_integration.py                   # Reasoning models (FIXED)
    ‚îú‚îÄ‚îÄ test_retry_logic.py                             # Retry decorator (FIXED)
    ‚îî‚îÄ‚îÄ ... (23 other standalone tests)
```

---

## Prerequisites & Environment Setup

### 1. System Requirements
- **LM Studio**: Running with API server on http://localhost:1234
- **Python**: 3.9+ with all dependencies installed
- **lms CLI**: Optional but recommended for model management
  ```bash
  brew install lmstudio-ai/lmstudio/lms
  ```

### 2. Model Requirements

**CRITICAL**: Tests depend on model availability and type

#### Minimum Models Needed
1. **Chat Model** (any one):
   - `qwen/qwen3-coder-30b` (recommended)
   - `ibm/granite-4-h-tiny`
   - Any other chat-capable model

2. **Reasoning Model** (for reasoning tests):
   - `mistralai/magistral-small-2509` (recommended)
   - `qwen/qwen3-4b-thinking-2507`
   - `deepseek/deepseek-r1-0528-qwen3-8b`

3. **Embedding Model** (for embeddings tests):
   - `text-embedding-qwen3-embedding-8b`
   - `text-embedding-nomic-embed-text-v2-moe`
   - Any model with "embedding" in name

#### Check Current Model Status
```bash
lms ps                    # List loaded models
lms ls                    # List available models
```

### 3. Environment Variables
```bash
# Optional - defaults to localhost:1234
export LM_STUDIO_API_BASE="http://localhost:1234/v1"
```

### 4. Python Dependencies
```bash
pip install -r requirements.txt
# or
uv pip install -e .
```

---

## Execution Order

### Recommended Test Execution Order

**IMPORTANT**: Some tests have implicit ordering dependencies

#### Phase 1: Basic Connectivity (Run First)
```bash
# 1. Verify LM Studio is running
curl http://localhost:1234/v1/models

# 2. Test basic API connectivity
python3 test_all_apis_comprehensive.py
```

#### Phase 2: Core Functionality Tests
```bash
# 3. Error handling and retry logic
python3 test_retry_logic.py
python3 test_llmclient_error_handling_integration.py

# 4. MCP tool registration
python3 test_mcp_tool_model_parameter_support.py

# 5. Reasoning integration (requires reasoning model)
python3 test_reasoning_integration.py
```

#### Phase 3: Full Test Suite
```bash
# 6. Run all standalone tests
./run_all_standalone_tests.sh

# 7. Run pytest suite
python3 -m pytest tests/ -v
```

---

## Test Categories

### 1. API Integration Tests
**Purpose**: Verify LM Studio API endpoints work correctly

- `test_all_apis_comprehensive.py` ‚≠ê **FIXED**
  - Tests all 5 OpenAI-compatible endpoints
  - Handles missing embedding models gracefully
  - **Prerequisite**: Any chat model loaded
  - **Optional**: Embedding model for full coverage

### 2. Error Handling Tests  
**Purpose**: Verify exception hierarchy and retry logic

- `test_retry_logic.py` ‚≠ê **FIXED**
  - Tests @retry_with_backoff decorator
  - Verifies exponential backoff behavior
  - **Key Discovery**: Decorator config at method level, NOT per-call

- `test_llmclient_error_handling_integration.py` ‚≠ê **FIXED, RENAMED**
  - Was: test_phase2_3.py
  - Tests exception imports and usage
  - Verifies docstring documentation
  - **Prerequisite**: None (static analysis)

### 3. MCP Tool Tests
**Purpose**: Verify MCP tool registration and multi-model support

- `test_mcp_tool_model_parameter_support.py` ‚≠ê **FIXED, RENAMED**
  - Was: test_phase2_2.py
  - Tests optional model parameter in @mcp.tool() functions
  - Uses AST parsing for validation
  - **Key Discovery**: async functions create AsyncFunctionDef, not FunctionDef
  - **Prerequisite**: None (static analysis)

### 4. Reasoning Model Tests
**Purpose**: Verify reasoning display and model-specific features

- `test_reasoning_integration.py` ‚≠ê **FIXED**
  - Tests reasoning_content field parsing
  - Tests HTML escaping and truncation
  - **Prerequisite**: Reasoning model loaded (magistral-small-2509)
  - **Key Discovery**: Must specify model explicitly, never assume default

### 5. Model Management Tests (NEW)
**Purpose**: Ensure models are loaded before tests run

- `tests/fixtures/model_management.py` ‚≠ê **NEW**
  - Provides pytest fixtures for model loading
  - Functions: `ensure_model_loaded()`, `get_default_model()`
  - Fixtures: `require_qwen_coder`, `require_magistral`, `require_any_model`
  - **Usage**:
    ```python
    @pytest.mark.usefixtures("require_qwen_coder")
    def test_something():
        # Test runs only if qwen/qwen3-coder-30b is loaded
        pass
    ```

---

## Critical Discoveries & Lessons

### Discovery 1: Test vs Code Correctness ‚ö†Ô∏è
**Test**: test_retry_logic.py  
**Issue**: Test was passing invalid parameters  
**Lesson**: Always verify test assumptions against actual code
```python
# ‚ùå WRONG - Test assumed per-call retry config
result = client.create_response(
    input_text="Test",
    max_retries=2,        # Does NOT exist
    retry_delay=0.1       # Does NOT exist
)

# ‚úÖ CORRECT - Decorator configured at method level
@retry_with_backoff(
    max_retries=DEFAULT_MAX_RETRIES + 1,
    base_delay=DEFAULT_RETRY_DELAY
)
def create_response(self, input_text, ...):
    pass
```

### Discovery 2: Model Loading State üî¥ **CRITICAL**
**Problem**: Tests fail when models unloaded after long test runs  
**Lesson**: Never assume a model is loaded

```bash
# After running 28 tests:
$ lms ps
Error: No models are currently loaded

# Solution: Check before testing
if not ensure_model_loaded("qwen/qwen3-coder-30b"):
    pytest.skip("Model not loaded")
```

### Discovery 3: AST Parsing Pitfalls üêç
**Test**: test_mcp_tool_model_parameter_support.py  
**Issue**: Async functions create different AST nodes  
**Lesson**: Handle both FunctionDef and AsyncFunctionDef

```python
# ‚ùå WRONG - Misses async functions
if isinstance(node, ast.FunctionDef):
    tool_functions.append(node)

# ‚úÖ CORRECT - Handles both types
if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
    tool_functions.append(node)
```

### Discovery 4: Exception Chain Swallowing üîó
**Test**: test_all_apis_comprehensive.py  
**Issue**: Retry decorator swallows __cause__ chain  
**Lesson**: Check error message directly as fallback

```python
# ‚ùå UNRELIABLE - __cause__ may be None
if "404" in str(api_error.__cause__):
    skip_test()

# ‚úÖ RELIABLE - Check message directly
error_str = str(api_error)
if "404" in error_str or "embeddings" in error_str.lower():
    skip_test()
```

### Discovery 5: Descriptive Naming Matters üìù
**Issue**: test_phase2_2.py and test_phase2_3.py revealed nothing about purpose  
**Lesson**: Name tests after what they verify

```
# ‚ùå BAD
test_phase2_2.py
test_phase2_3.py

# ‚úÖ GOOD  
test_mcp_tool_model_parameter_support.py
test_llmclient_error_handling_integration.py
```

### Discovery 6: Embedding Models Are Special üéØ
**Test**: test_all_apis_comprehensive.py  
**Issue**: /v1/embeddings requires embedding-specific model  
**Lesson**: API endpoints have model type requirements

```python
# Chat models ‚â† Embedding models
chat_models = ["qwen3-coder", "granite"]          # Cannot generate embeddings
embedding_models = ["qwen3-embedding-8b"]         # Cannot chat

# LM Studio returns 404 if wrong model type loaded
```

---

## Model Management

### Model Loading Best Practices

#### 1. Always Check Before Testing
```python
from tests.fixtures.model_management import ensure_model_loaded

# Option A: Pytest fixture (automatic skip)
@pytest.mark.usefixtures("require_qwen_coder")
def test_something():
    pass

# Option B: Manual check (custom handling)
def test_something():
    if not ensure_model_loaded("qwen/qwen3-coder-30b"):
        pytest.skip("Model not available")
```

#### 2. Load Models Strategically
```bash
# For maximum test coverage, load:
lms load qwen/qwen3-coder-30b              # Chat tests
lms load mistralai/magistral-small-2509    # Reasoning tests  
lms load text-embedding-qwen3-embedding-8b # Embedding tests

# Check status
lms ps
```

#### 3. Unload Models to Save Memory
```bash
# Unload all
lms unload --all

# Unload specific
lms unload qwen/qwen3-coder-30b
```

### Model Type Requirements by Test

| Test | Required Model Type | Specific Model | Can Skip? |
|------|---------------------|----------------|-----------|
| test_all_apis_comprehensive.py | Any chat | Any | No |
| test_reasoning_integration.py | Reasoning | magistral-small-2509 | No |
| test_mcp_tool_model_parameter_support.py | None | - | N/A |
| test_retry_logic.py | None | - | N/A |
| test_llmclient_error_handling_integration.py | None | - | N/A |

---

## Troubleshooting

### Common Issues

#### Issue 1: "No models are currently loaded"
**Symptoms**: Tests fail with connection errors  
**Cause**: LM Studio has no active model  
**Solution**:
```bash
lms ps                           # Check status
lms load qwen/qwen3-coder-30b   # Load a model
```

#### Issue 2: "404 Not Found" on /v1/embeddings
**Symptoms**: test_all_apis_comprehensive.py fails on Test 5  
**Cause**: No embedding model loaded  
**Solution**: Test now skips gracefully (FIXED)
```bash
# To run full test:
lms load text-embedding-qwen3-embedding-8b
```

#### Issue 3: "AsyncFunctionDef has no attribute X"
**Symptoms**: AST parsing fails in test_mcp_tool_model_parameter_support.py  
**Cause**: Code uses async functions but test checks for FunctionDef  
**Solution**: Already fixed - test handles both types

#### Issue 4: Tests pass individually but fail in batch
**Symptoms**: Tests pass one-by-one but fail in ./run_all_standalone_tests.sh  
**Cause**: Model gets unloaded after multiple tests  
**Solution**: Use fixtures or check model before each test
```python
@pytest.fixture(autouse=True)
def ensure_model():
    ensure_model_loaded("qwen/qwen3-coder-30b")
```

---

## Test Execution Examples

### Quick Health Check
```bash
# Test LM Studio connectivity only
python3 test_all_apis_comprehensive.py
```

### Full Standalone Suite
```bash
# Run all 28 standalone tests
./run_all_standalone_tests.sh

# Expected output:
# Total Scripts: 28
# Passed: 28 ‚úÖ
# Failed: 0 ‚ùå
```

### Full Pytest Suite
```bash
# Run all pytest tests
python3 -m pytest tests/ -v

# Expected output:
# 172 tests: 170 passed, 2 failed (known issues)
```

### Specific Test Category
```bash
# Error handling only
python3 test_retry_logic.py
python3 test_llmclient_error_handling_integration.py

# MCP tools only
python3 test_mcp_tool_model_parameter_support.py

# Reasoning only
python3 test_reasoning_integration.py
```

---

## Future Test Development Guidelines

### 1. Always Include Prerequisites
```python
def test_something():
    """Test XYZ functionality.
    
    Prerequisites:
        - Model: qwen/qwen3-coder-30b (chat)
        - LM Studio: Running on localhost:1234
        - No other requirements
    """
```

### 2. Handle Environmental Variations
```python
# Good: Graceful degradation
try:
    result = test_with_model()
except ModelNotFound:
    pytest.skip("Optional model not available")

# Bad: Assume environment
result = test_with_model()  # Crashes if model missing
```

### 3. Use Descriptive Names
```python
# Good
test_retry_decorator_exponential_backoff.py
test_magistral_reasoning_content_parsing.py

# Bad
test_phase3.py
test_feature_x.py
```

### 4. Document Model Requirements
```python
# At top of test file
"""
Model Requirements:
    - Primary: qwen/qwen3-coder-30b (required)
    - Optional: mistralai/magistral-small-2509 (for extended tests)
    
Test Coverage:
    - Basic functionality: No special model needed
    - Reasoning tests: Requires magistral (skips if not available)
"""
```

---

## Statistics

### Current Test Status
- **Standalone Tests**: 28/28 passing (100%)
- **Pytest Suite**: 170/172 passing (98.8%)
- **Total Test Cases**: 200+ individual assertions
- **Test Files**: 38 files (10 pytest + 28 standalone)

### Test Fix History
- **November 2, 2025**: Fixed 5 standalone tests
  - test_retry_logic.py (4 tests)
  - test_reasoning_integration.py (1 test)
  - test_mcp_tool_model_parameter_support.py (22 tests)
  - test_llmclient_error_handling_integration.py (22 tests)
  - test_all_apis_comprehensive.py (5 tests)

---

**Maintained by**: Claude Code  
**Last Test Run**: November 2, 2025  
**Test Runner**: ./run_all_standalone_tests.sh + pytest
