# Option A: Hardened MVP Implementation Plan

**Multi-Model Support for LM Studio Bridge Enhanced**

---

## âš ï¸ CRITICAL: Complete Phase 0-1 FIRST

**Status**: ðŸ”´ **BLOCKED** - Cannot start until Phase 0-1 complete

After comprehensive testing and Qwen 3 code review (October 30, 2025), **critical production-blocking issues** were identified that MUST be fixed before implementing multi-model support.

**Required Prerequisite**: Complete `PHASE_0_1_QWEN_CRITICAL_FIXES.md` (8-10 hours)
- Phase 0: Critical Production Fixes (3-4h) - TTL, health checks, retry logic
- Phase 1: Production Hardening (5-6h) - Failure tests, benchmarks, observability

**Current Production Readiness**: âŒ NOT READY (Qwen rating: 6/10)
**After Phase 0-1**: âœ… READY (Target rating: 9/10)

**Do not proceed with Option A until Phase 0-1 is complete and approved by Qwen review.**

---

## Executive Summary

**Objective**: Add robust multi-model support to autonomous tools with production-ready validation and error handling.

**Approach**: Hardened MVP - Simple but solid implementation with proper validation, error handling, and testing.

**Timeline**: 8-10 hours (4 phases)

**Team**: Claude Code + 3 Local LLMs (Qwen3-Coder, Qwen3-Thinking, Magistral)

---

## Option A Goals

âœ… **Model parameter support** - Pass model to autonomous tools
âœ… **Validation layer** - Verify model exists before use
âœ… **Error handling framework** - Graceful failures with retry logic
âœ… **Backward compatibility** - Optional parameter, defaults work
âœ… **Production ready** - Tests, docs, logging
âœ… **Keep it simple** - No architectural changes, minimal complexity

---

## LLM Review Feedback & Critical Updates

**Review Date**: October 30, 2025
**Reviewers**: Qwen3-Coder, Qwen3-Thinking, Magistral

âš ï¸ **NOTE**: For Qwen's critical production-blocking findings (TTL configuration, health checks, retry logic, failure testing, performance benchmarks), see `PHASE_0_1_QWEN_CRITICAL_FIXES.md`. Those MUST be completed before starting Option A phases below.

### âœ… Critical Issues Fixed

#### 1. Previous Response ID Bug (COMPLETED âœ…)
**Issue**: `previous_response_id` was always null, breaking stateful conversations
**Fix Applied**: Updated both `_autonomous_loop` methods to use `create_response()` with proper `previous_response_id` tracking
**Impact**: 97% token savings restored, conversations maintain proper state

#### 2. Missing Imports in Code Examples
**Issue** (Qwen3-Coder): Code examples missing required imports
**Fix**: Added imports section to each code example

**Required imports for all modules**:
```python
# Standard library
from datetime import datetime
from functools import wraps
from typing import List, Optional, Dict, Any, Union
import asyncio
import logging

# Third-party
import httpx

# Local
from llm.exceptions import *
from config import get_config
```

#### 3. Backward Compatibility Plan
**Issue** (Qwen3-Thinking): No explicit backward compatibility section
**Fix**: Added comprehensive backward compatibility requirements

**Backward Compatibility Requirements**:
- âœ… `model=None` must work (uses default model from config)
- âœ… Existing code without `model` parameter continues to work
- âœ… All existing tests pass without modification
- âœ… No breaking changes to tool signatures (parameter is optional)
- âœ… Default behavior unchanged when parameter not provided

**Testing Strategy**:
```python
# Test 1: Without model parameter (backward compat)
result = await agent.autonomous_with_mcp(
    "filesystem",
    "List files"
    # No model parameter - should use default
)
assert result  # Should work

# Test 2: With model=None explicitly
result = await agent.autonomous_with_mcp(
    "filesystem",
    "List files",
    model=None  # Explicitly None - should use default
)
assert result  # Should work

# Test 3: With specific model
result = await agent.autonomous_with_mcp(
    "filesystem",
    "List files",
    model="qwen/qwen3-coder-30b"  # Specific model
)
assert result  # Should work with specified model
```

#### 4. Review Checkpoints Clarified
**Issue** (Qwen3-Thinking): No explicit review checkpoints after each phase
**Fix**: Added formal phase completion reviews

**Phase Completion Review Process**:
After each phase, ALL 3 LLMs must review and approve:
- âœ… All tasks in phase completed
- âœ… All acceptance criteria met
- âœ… All tests passing
- âœ… No regressions introduced
- âœ… Documentation updated
- âœ… Ready for next phase

**Sign-off required from**:
- Qwen3-Coder (code quality)
- Qwen3-Thinking (logical completeness)
- Magistral (architecture soundness)

#### 5. Multi-Model Edge Cases
**Issue** (Qwen3-Thinking): Missing edge case handling for concurrent model requests
**Fix**: Added edge case handling plan

**Edge Cases to Handle**:
1. **Concurrent requests with different models**: Out of scope for MVP (Phase 1 doesn't support concurrency)
2. **Model not loaded but exists**: ModelValidator returns clear error
3. **Model name typo**: ModelNotFoundError with list of available models
4. **Network failure during validation**: Retry with exponential backoff
5. **Model validation cache stale**: 60s TTL ensures freshness

**Future Enhancement** (Option C):
- Concurrent model execution with connection pooling
- Model selection strategies
- Automatic fallback to alternative models

#### 6. Phase 2 Timeline Adjustment
**Issue** (Qwen3-Coder): Phase 2 estimate too optimistic
**Fix**: Adjusted from 2.5-3h to 3-3.5h

**Revised Timeline**:
- Phase 1: 2-2.5h (unchanged)
- Phase 2: 3-3.5h (increased by 30-60 min)
- Phase 3: 1.5-2h (unchanged)
- Phase 4: 2-2.5h (unchanged)
- **New Total**: 9-11.5 hours (was 8-10 hours)

#### 7. Added Logging Throughout
**Issue** (Qwen3-Coder suggestion): Add traceability via logging
**Fix**: All code examples include `logger.info()` and `logger.error()` calls

---

## Updated Success Criteria

### Must Have (Critical)
- [x] `previous_response_id` bug fixed
- [ ] All imports defined in code examples
- [ ] Backward compatibility verified (3 test scenarios)
- [ ] Phase completion reviews after each phase
- [ ] Edge cases documented and handled
- [ ] All 3 LLMs approve final implementation

### Should Have (Important)
- [ ] Logging added throughout for traceability
- [ ] Model alias support (nice-to-have)
- [ ] Performance benchmarks vs old implementation

### Nice to Have (Optional)
- [ ] Cross-tool validation tests
- [ ] Concurrent request handling (defer to Option C)

---

## Phase 1: Model Validation Layer (2-2.5 hours)

âš ï¸ **PREREQUISITE**: Complete `PHASE_0_1_QWEN_CRITICAL_FIXES.md` before starting this phase.

### Overview
Create robust model validation infrastructure before touching core autonomous tools.

### Tasks

#### 1.1 Create Exception Hierarchy
**Owner**: Claude Code
**Reviewer**: Qwen3-Coder
**Time**: 30 minutes

**File**: `llm/exceptions.py` (NEW)

**Implementation**:
```python
"""Exception hierarchy for LLM operations."""

from datetime import datetime
from typing import Optional, List

class LLMError(Exception):
    """Base exception for LLM-related errors."""
    def __init__(self, message: str, original_exception: Optional[Exception] = None):
        super().__init__(message)
        self.original_exception = original_exception
        self.timestamp = datetime.utcnow()

class LLMTimeoutError(LLMError):
    """Raised when LLM request times out."""
    pass

class LLMRateLimitError(LLMError):
    """Raised when LLM rate limit is exceeded."""
    pass

class LLMValidationError(LLMError):
    """Raised when LLM response validation fails."""
    pass

class LLMConnectionError(LLMError):
    """Raised when LLM connection fails."""
    pass

class LLMResponseError(LLMError):
    """Raised when LLM response format is invalid."""
    pass

class ModelNotFoundError(LLMValidationError):
    """Raised when requested model is not available."""
    def __init__(self, model_name: str, available_models: List[str]):
        self.model_name = model_name
        self.available_models = available_models
        message = f"Model '{model_name}' not found. Available: {', '.join(available_models)}"
        super().__init__(message)
```

**Acceptance Criteria**:
- [ ] All 6 exception classes defined
- [ ] Base class has original_exception attribute
- [ ] ModelNotFoundError includes available_models
- [ ] Docstrings complete

**Review Checkpoint**: Qwen3-Coder reviews exception hierarchy design

---

#### 1.2 Create Error Handling Utilities
**Owner**: Claude Code
**Reviewer**: Qwen3-Thinking
**Time**: 45 minutes

**File**: `utils/error_handling.py` (NEW)

**Implementation**:
```python
"""Error handling utilities for LLM operations."""

import time
from functools import wraps
from typing import Callable, Any, Optional
import logging

logger = logging.getLogger(__name__)

def retry_with_backoff(
    max_retries: int = 3,
    base_delay: float = 1.0,
    max_delay: float = 60.0,
    exceptions: tuple = (Exception,)
):
    """
    Retry decorator with exponential backoff.

    Args:
        max_retries: Maximum number of retry attempts
        base_delay: Initial delay in seconds
        max_delay: Maximum delay in seconds
        exceptions: Tuple of exceptions to catch and retry

    Returns:
        Decorated function with retry logic
    """
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        async def async_wrapper(*args, **kwargs) -> Any:
            last_exception = None
            for attempt in range(max_retries):
                try:
                    return await func(*args, **kwargs)
                except exceptions as e:
                    last_exception = e
                    if attempt == max_retries - 1:
                        logger.error(f"Max retries ({max_retries}) reached for {func.__name__}")
                        raise

                    delay = min(base_delay * (2 ** attempt), max_delay)
                    logger.warning(f"Attempt {attempt + 1} failed, retrying in {delay}s: {e}")
                    await asyncio.sleep(delay)

            raise last_exception

        @wraps(func)
        def sync_wrapper(*args, **kwargs) -> Any:
            last_exception = None
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except exceptions as e:
                    last_exception = e
                    if attempt == max_retries - 1:
                        logger.error(f"Max retries ({max_retries}) reached for {func.__name__}")
                        raise

                    delay = min(base_delay * (2 ** attempt), max_delay)
                    logger.warning(f"Attempt {attempt + 1} failed, retrying in {delay}s: {e}")
                    time.sleep(delay)

            raise last_exception

        return async_wrapper if asyncio.iscoroutinefunction(func) else sync_wrapper

    return decorator


def fallback_strategy(
    fallback_func: Callable,
    fallback_args: Optional[tuple] = None,
    fallback_kwargs: Optional[dict] = None
):
    """
    Decorator that provides fallback when main function fails.

    Args:
        fallback_func: Function to call on failure
        fallback_args: Args for fallback function
        fallback_kwargs: Kwargs for fallback function

    Returns:
        Decorated function with fallback logic
    """
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        async def wrapper(*args, **kwargs) -> Any:
            try:
                return await func(*args, **kwargs)
            except Exception as e:
                logger.warning(f"{func.__name__} failed, using fallback: {e}")
                f_args = fallback_args or ()
                f_kwargs = fallback_kwargs or {}
                return await fallback_func(*f_args, **f_kwargs)

        return wrapper

    return decorator
```

**Acceptance Criteria**:
- [ ] retry_with_backoff supports both async and sync
- [ ] Exponential backoff calculation correct
- [ ] Logging at appropriate levels
- [ ] fallback_strategy decorator works
- [ ] Type hints complete

**Review Checkpoint**: Qwen3-Thinking reviews error handling patterns

---

#### 1.3 Implement Model Validator
**Owner**: Qwen3-Coder (design) â†’ Claude Code (implementation)
**Reviewer**: Magistral
**Time**: 45 minutes

**File**: `llm/model_validator.py` (NEW)

**Design Requirements** (from Qwen3-Coder):
1. Fetch available models from LM Studio API
2. Cache model list (TTL: 60 seconds)
3. Validate model name against cached list
4. Return clear error with available models if not found
5. Handle API failures gracefully

**Implementation**:
```python
"""Model validation for LM Studio."""

import asyncio
from typing import List, Optional
from datetime import datetime, timedelta
import logging
import httpx

from llm.exceptions import ModelNotFoundError, LLMConnectionError
from utils.error_handling import retry_with_backoff
from config import get_config

logger = logging.getLogger(__name__)

class ModelValidator:
    """Validates model availability against LM Studio API."""

    def __init__(self, api_base: Optional[str] = None):
        config = get_config()
        self.api_base = api_base or config.lmstudio.api_base
        self._cache: Optional[List[str]] = None
        self._cache_timestamp: Optional[datetime] = None
        self._cache_ttl = timedelta(seconds=60)

    @retry_with_backoff(max_retries=3, base_delay=1.0)
    async def _fetch_models(self) -> List[str]:
        """Fetch available models from LM Studio API."""
        try:
            async with httpx.AsyncClient(timeout=10.0) as client:
                response = await client.get(f"{self.api_base}/v1/models")
                response.raise_for_status()
                data = response.json()

                # Extract model IDs from response
                models = [model["id"] for model in data.get("data", [])]
                logger.debug(f"Fetched {len(models)} models from LM Studio")
                return models

        except httpx.HTTPError as e:
            logger.error(f"Failed to fetch models from LM Studio: {e}")
            raise LLMConnectionError(f"Could not connect to LM Studio API", e)

    async def get_available_models(self, use_cache: bool = True) -> List[str]:
        """
        Get list of available models.

        Args:
            use_cache: Whether to use cached model list

        Returns:
            List of available model IDs
        """
        now = datetime.utcnow()

        # Check cache validity
        if use_cache and self._cache is not None and self._cache_timestamp is not None:
            if now - self._cache_timestamp < self._cache_ttl:
                logger.debug("Using cached model list")
                return self._cache

        # Fetch fresh model list
        models = await self._fetch_models()
        self._cache = models
        self._cache_timestamp = now

        return models

    async def validate_model(self, model_name: Optional[str]) -> bool:
        """
        Validate if model exists in LM Studio.

        Args:
            model_name: Model ID to validate (None means use default)

        Returns:
            True if model exists or None (default)

        Raises:
            ModelNotFoundError: If model not found
        """
        # None means use default model (always valid)
        if model_name is None or model_name == "default":
            return True

        available_models = await self.get_available_models()

        if model_name not in available_models:
            raise ModelNotFoundError(model_name, available_models)

        logger.info(f"Model '{model_name}' validated successfully")
        return True

    def clear_cache(self):
        """Clear model cache."""
        self._cache = None
        self._cache_timestamp = None
        logger.debug("Model cache cleared")
```

**Acceptance Criteria**:
- [ ] Fetches models from /v1/models endpoint
- [ ] Caches model list for 60 seconds
- [ ] Validates model name correctly
- [ ] Raises ModelNotFoundError with available models
- [ ] Handles API failures gracefully
- [ ] Logging at appropriate levels

**Review Checkpoint**: Magistral reviews validation architecture

---

#### 1.4 Create Tests for Validation Layer
**Owner**: Claude Code
**Reviewer**: Qwen3-Thinking
**Time**: 30 minutes

**Files**:
- `tests/test_exceptions.py` (NEW)
- `tests/test_error_handling.py` (NEW)
- `tests/test_model_validator.py` (NEW)

**Test Coverage Requirements**:

`tests/test_exceptions.py`:
```python
"""Tests for LLM exception hierarchy."""

import pytest
from llm.exceptions import (
    LLMError, LLMTimeoutError, ModelNotFoundError
)

def test_base_exception_stores_original():
    """Base exception should store original exception."""
    original = ValueError("original error")
    error = LLMError("wrapped", original)
    assert error.original_exception is original

def test_model_not_found_includes_available():
    """ModelNotFoundError should include available models."""
    available = ["model1", "model2"]
    error = ModelNotFoundError("model3", available)
    assert error.model_name == "model3"
    assert error.available_models == available
    assert "model1" in str(error)

# ... 5 more tests
```

`tests/test_error_handling.py`:
```python
"""Tests for error handling utilities."""

import pytest
import asyncio
from utils.error_handling import retry_with_backoff, fallback_strategy

@pytest.mark.asyncio
async def test_retry_success_on_second_attempt():
    """Should retry and succeed on second attempt."""
    attempts = []

    @retry_with_backoff(max_retries=3, base_delay=0.1)
    async def flaky_function():
        attempts.append(1)
        if len(attempts) < 2:
            raise ValueError("Temporary error")
        return "success"

    result = await flaky_function()
    assert result == "success"
    assert len(attempts) == 2

# ... 8 more tests
```

`tests/test_model_validator.py`:
```python
"""Tests for model validator."""

import pytest
from llm.model_validator import ModelValidator
from llm.exceptions import ModelNotFoundError

@pytest.mark.asyncio
async def test_validate_existing_model():
    """Should validate existing model successfully."""
    validator = ModelValidator()
    # Assumes LM Studio running with at least one model
    models = await validator.get_available_models()
    if models:
        result = await validator.validate_model(models[0])
        assert result is True

@pytest.mark.asyncio
async def test_validate_nonexistent_model_raises():
    """Should raise ModelNotFoundError for invalid model."""
    validator = ModelValidator()
    with pytest.raises(ModelNotFoundError) as exc_info:
        await validator.validate_model("nonexistent-model-xyz")

    assert "nonexistent-model-xyz" in str(exc_info.value)
    assert len(exc_info.value.available_models) > 0

# ... 7 more tests
```

**Acceptance Criteria**:
- [ ] All exception classes tested
- [ ] Retry logic tested (success, max retries, exponential backoff)
- [ ] Fallback strategy tested
- [ ] Model validation tested (valid, invalid, None/default)
- [ ] Cache behavior tested
- [ ] Test coverage > 90%

**Review Checkpoint**: Qwen3-Thinking reviews test completeness

---

### Phase 1 Completion Review

**Reviewer**: All 3 LLMs
**Time**: 15 minutes

**Checklist**:
- [ ] Exception hierarchy complete and logical
- [ ] Error handling utilities work correctly
- [ ] Model validator implemented and tested
- [ ] All tests pass
- [ ] Code quality meets standards
- [ ] Documentation complete

**Deliverables**:
- `llm/exceptions.py`
- `utils/error_handling.py`
- `llm/model_validator.py`
- `tests/test_exceptions.py`
- `tests/test_error_handling.py`
- `tests/test_model_validator.py`

---

## Phase 2: Core Tool Interface Updates (2.5-3 hours)

### Overview
Add model parameter to autonomous tools with proper validation and error handling.

### Tasks

#### 2.1 Update DynamicAutonomousAgent Class
**Owner**: Claude Code
**Reviewer**: Qwen3-Coder
**Time**: 1 hour

**File**: `tools/dynamic_autonomous.py`

**Changes Required**:

1. Add ModelValidator import and initialization:
```python
from llm.model_validator import ModelValidator
from llm.exceptions import ModelNotFoundError, LLMConnectionError

class DynamicAutonomousAgent:
    def __init__(self, mcp_discovery=None, llm_client=None):
        # ... existing code ...
        self.model_validator = ModelValidator()
```

2. Update `autonomous_with_mcp` method signature and implementation:
```python
async def autonomous_with_mcp(
    self,
    mcp_name: str,
    task: str,
    max_rounds: int = DEFAULT_MAX_ROUNDS,
    max_tokens: Union[int, str] = "auto",
    model: Optional[str] = None  # NEW PARAMETER
) -> str:
    """
    Execute task autonomously using single MCP with optional model selection.

    Args:
        mcp_name: Name of MCP to use
        task: Task description
        max_rounds: Maximum autonomous loop iterations
        max_tokens: Maximum tokens per response
        model: Optional model to use (None uses default)

    Returns:
        Final answer from LLM

    Raises:
        ModelNotFoundError: If specified model not available
        LLMConnectionError: If cannot connect to LM Studio
    """
    try:
        # Validate model if specified
        if model is not None:
            await self.model_validator.validate_model(model)
            logger.info(f"Using model: {model}")

        # Create fresh discovery (hot reload)
        discovery = MCPDiscovery(self.mcp_json_path)

        # Get connection params
        params = discovery.get_connection_params(mcp_name)

        # Create LLM client with specific model or use default
        llm = LLMClient(model=model) if model else self.llm

        # Rest of implementation remains the same
        # ... (autonomous loop logic unchanged)

    except ModelNotFoundError as e:
        logger.error(f"Model validation failed: {e}")
        return f"Error: {str(e)}"
    except LLMConnectionError as e:
        logger.error(f"LM Studio connection failed: {e}")
        return f"Error: Could not connect to LM Studio. Is it running?"
    except Exception as e:
        logger.exception(f"Unexpected error in autonomous_with_mcp")
        return f"Error: {str(e)}"
```

3. Update `autonomous_with_multiple_mcps` method (similar pattern):
```python
async def autonomous_with_multiple_mcps(
    self,
    mcp_names: List[str],
    task: str,
    max_rounds: int = DEFAULT_MAX_ROUNDS,
    max_tokens: Union[int, str] = "auto",
    model: Optional[str] = None  # NEW PARAMETER
) -> str:
    """Execute task using multiple MCPs with optional model selection."""
    # Same validation pattern as above
    if model is not None:
        await self.model_validator.validate_model(model)

    llm = LLMClient(model=model) if model else self.llm

    # ... rest unchanged
```

4. Update `autonomous_discover_and_execute` method (similar pattern):
```python
async def autonomous_discover_and_execute(
    self,
    task: str,
    max_rounds: int = DEFAULT_MAX_ROUNDS,
    max_tokens: Union[int, str] = "auto",
    model: Optional[str] = None  # NEW PARAMETER
) -> str:
    """Execute task with ALL MCPs and optional model selection."""
    # Same validation pattern
```

**Acceptance Criteria**:
- [ ] All 3 methods have model parameter
- [ ] Model validation called before LLMClient creation
- [ ] Clear error messages for validation failures
- [ ] Backward compatible (model=None works)
- [ ] Logging includes model name
- [ ] Docstrings updated

**Review Checkpoint**: Qwen3-Coder reviews implementation

---

#### 2.2 Update Tool Registration
**Owner**: Claude Code
**Reviewer**: Qwen3-Thinking
**Time**: 45 minutes

**File**: `tools/dynamic_autonomous_register.py`

**Changes Required**:

Update all 3 tool signatures to expose model parameter:

```python
@mcp.tool()
async def autonomous_with_mcp(
    mcp_name: Annotated[str, Field(
        description="Name of the MCP to use (e.g., 'filesystem', 'memory', 'fetch', 'github')"
    )],
    task: Annotated[str, Field(
        description="Task for the local LLM to execute autonomously",
        min_length=1,
        max_length=10000
    )],
    max_rounds: Annotated[int, Field(
        description="Maximum rounds for autonomous loop (default: 10000, no artificial limit)",
        ge=1
    )] = DEFAULT_MAX_ROUNDS,
    max_tokens: Annotated[Union[int, str], Field(
        description="Maximum tokens per LLM response ('auto' for default, or integer to override)"
    )] = "auto",
    model: Annotated[Optional[str], Field(
        description="Optional: Model to use for this task (e.g., 'qwen/qwen3-coder-30b'). If not specified, uses default model from config."
    )] = None  # NEW PARAMETER
) -> str:
    """
    Execute task autonomously using tools from a SINGLE MCP with optional model selection.

    ... (existing docstring content) ...

    Args:
        mcp_name: Name of the MCP to use
        task: Task description
        max_rounds: Maximum rounds (default: 10000)
        max_tokens: Maximum tokens per response ('auto' or integer)
        model: Optional model to use (None uses default)

    ... (rest of docstring) ...
    """
    return await agent.autonomous_with_mcp(
        mcp_name=mcp_name,
        task=task,
        max_rounds=max_rounds,
        max_tokens=max_tokens,
        model=model  # NEW PARAMETER
    )

# Repeat for autonomous_with_multiple_mcps and autonomous_discover_and_execute
```

**Acceptance Criteria**:
- [ ] All 3 tool functions have model parameter
- [ ] Parameter descriptions clear and helpful
- [ ] Field annotations correct
- [ ] Docstrings updated with examples
- [ ] Parameter passed to agent methods

**Review Checkpoint**: Qwen3-Thinking reviews API design

---

#### 2.3 Update LLMClient Error Handling
**Owner**: Claude Code
**Reviewer**: Qwen3-Coder
**Time**: 30 minutes

**File**: `llm/llm_client.py`

**Changes Required**:

1. Add exception imports:
```python
from llm.exceptions import (
    LLMConnectionError, LLMTimeoutError, LLMResponseError
)
from utils.error_handling import retry_with_backoff
```

2. Add retry decorator to chat_completion:
```python
@retry_with_backoff(
    max_retries=3,
    base_delay=1.0,
    exceptions=(httpx.TimeoutException, httpx.ConnectError)
)
async def chat_completion(
    self,
    messages: list,
    temperature: float = 0.7,
    max_tokens: int = DEFAULT_MAX_TOKENS,
    tools: Optional[list] = None,
    tool_choice: Optional[str] = None,
) -> dict:
    """Chat completion with automatic retry on transient failures."""
    try:
        # ... existing implementation ...

    except httpx.TimeoutException as e:
        logger.error(f"LM Studio request timed out: {e}")
        raise LLMTimeoutError("Request timed out", e)
    except httpx.ConnectError as e:
        logger.error(f"Could not connect to LM Studio: {e}")
        raise LLMConnectionError("Connection failed", e)
    except Exception as e:
        logger.exception("Unexpected error in chat_completion")
        raise LLMResponseError(f"Unexpected error: {str(e)}", e)
```

**Acceptance Criteria**:
- [ ] Retry decorator applied
- [ ] Specific exceptions raised
- [ ] Error messages clear
- [ ] Original exception preserved
- [ ] Logging appropriate

**Review Checkpoint**: Qwen3-Coder reviews error handling

---

#### 2.4 Integration Testing
**Owner**: Claude Code
**Reviewer**: Magistral
**Time**: 45 minutes

**File**: `tests/test_multi_model_integration.py` (NEW)

**Test Scenarios**:

```python
"""Integration tests for multi-model support."""

import pytest
import asyncio
from tools.dynamic_autonomous import DynamicAutonomousAgent
from llm.exceptions import ModelNotFoundError

@pytest.mark.asyncio
async def test_autonomous_with_specific_model():
    """Should execute task with specified model."""
    agent = DynamicAutonomousAgent()

    # Get available models first
    models = await agent.model_validator.get_available_models()
    if not models:
        pytest.skip("No models available in LM Studio")

    # Use first available model
    result = await agent.autonomous_with_mcp(
        mcp_name="filesystem",
        task="List files in current directory and count them",
        max_rounds=20,
        model=models[0]
    )

    assert result is not None
    assert "Error" not in result or "files" in result.lower()

@pytest.mark.asyncio
async def test_autonomous_with_invalid_model():
    """Should fail gracefully with invalid model."""
    agent = DynamicAutonomousAgent()

    result = await agent.autonomous_with_mcp(
        mcp_name="filesystem",
        task="Test task",
        model="nonexistent-model-xyz"
    )

    assert "Error" in result
    assert "not found" in result.lower()

@pytest.mark.asyncio
async def test_autonomous_without_model_uses_default():
    """Should use default model when model=None."""
    agent = DynamicAutonomousAgent()

    result = await agent.autonomous_with_mcp(
        mcp_name="filesystem",
        task="What is the current directory?",
        max_rounds=10
        # model=None (default)
    )

    assert result is not None
    # Should work with default model

@pytest.mark.asyncio
async def test_multiple_mcps_with_model():
    """Should work with multiple MCPs and specific model."""
    agent = DynamicAutonomousAgent()

    models = await agent.model_validator.get_available_models()
    if not models:
        pytest.skip("No models available")

    result = await agent.autonomous_with_multiple_mcps(
        mcp_names=["filesystem", "memory"],
        task="Read current directory and create a knowledge graph entity about the project",
        max_rounds=30,
        model=models[0]
    )

    assert result is not None

# ... 5 more integration tests
```

**Acceptance Criteria**:
- [ ] Tests cover all 3 autonomous methods
- [ ] Tests valid model, invalid model, None/default
- [ ] Tests error messages
- [ ] Tests backward compatibility
- [ ] All tests pass
- [ ] Test coverage > 85%

**Review Checkpoint**: Magistral reviews test coverage and scenarios

---

### Phase 2 Completion Review

**Reviewer**: All 3 LLMs
**Time**: 15 minutes

**Checklist**:
- [ ] Model parameter added to all tools
- [ ] Validation implemented correctly
- [ ] Error handling robust
- [ ] Backward compatible
- [ ] Integration tests pass
- [ ] Documentation complete

**Deliverables**:
- Updated `tools/dynamic_autonomous.py`
- Updated `tools/dynamic_autonomous_register.py`
- Updated `llm/llm_client.py`
- `tests/test_multi_model_integration.py`

---

## Phase 3: Documentation & Examples (1.5-2 hours)

### Overview
Comprehensive documentation for multi-model feature with examples and troubleshooting.

### Tasks

#### 3.1 Update API Reference
**Owner**: Claude Code
**Reviewer**: Qwen3-Thinking
**Time**: 30 minutes

**File**: `docs/API_REFERENCE.md`

**Updates Required**:

1. Add model parameter to all tool signatures
2. Add examples using model parameter
3. Document available models discovery
4. Add troubleshooting section

**Example Addition**:
```markdown
### Using Specific Models

All autonomous tools now support an optional `model` parameter:

**Example: Use reasoning model for RAG task**
```python
# Use reasoning model for exploration
autonomous_with_mcp(
    mcp_name="filesystem",
    task="Analyze codebase structure and identify key patterns",
    model="mistralai/magistral-small-2509"
)
```

**Example: Use coding model for implementation**
```python
# Use coding model for implementation
autonomous_with_mcp(
    mcp_name="filesystem",
    task="Implement helper function based on analysis",
    model="qwen/qwen3-coder-30b"
)
```

**Discovering Available Models**
```python
# List available models in LM Studio
list_models()
```
```

**Acceptance Criteria**:
- [ ] All tool signatures updated
- [ ] Model parameter documented
- [ ] Examples clear and practical
- [ ] Troubleshooting section added

**Review Checkpoint**: Qwen3-Thinking reviews clarity and completeness

---

#### 3.2 Update README
**Owner**: Claude Code
**Reviewer**: Qwen3-Coder
**Time**: 30 minutes

**File**: `README.md`

**Updates Required**:

1. Add "Multi-Model Support" section to features
2. Add quickstart example
3. Update use cases section

**Example Addition**:
```markdown
## Multi-Model Support âœ¨

Use different models for different tasks in the same workflow!

### Quick Example

```python
# Step 1: Use reasoning model to explore
autonomous_with_mcp(
    "filesystem",
    "Analyze project structure",
    model="mistralai/magistral-small-2509"
)

# Step 2: Use coding model to implement
autonomous_with_mcp(
    "filesystem",
    "Generate utility functions",
    model="qwen/qwen3-coder-30b"
)
```

### Benefits

- **Specialized models** - Use reasoning models for planning, coding models for implementation
- **Performance** - Choose faster models for simple tasks, powerful models for complex ones
- **Cost optimization** - Balance speed vs capability
- **Experimentation** - Compare different models on same task
```

**Acceptance Criteria**:
- [ ] Multi-model section prominent
- [ ] Examples practical and clear
- [ ] Benefits explained
- [ ] Quick to understand

**Review Checkpoint**: Qwen3-Coder reviews presentation

---

#### 3.3 Create Multi-Model Guide
**Owner**: Qwen3-Thinking (outline) â†’ Claude Code (implementation)
**Reviewer**: Magistral
**Time**: 45 minutes

**File**: `docs/MULTI_MODEL_GUIDE.md` (NEW)

**Outline** (from Qwen3-Thinking):

```markdown
# Multi-Model Usage Guide

## Table of Contents
1. Overview
2. When to Use Multiple Models
3. Available Models
4. Practical Examples
5. Best Practices
6. Performance Considerations
7. Troubleshooting

## 1. Overview
What is multi-model support and why it's useful...

## 2. When to Use Multiple Models

### Scenario 1: RAG + Implementation
- Reasoning model explores codebase
- Coding model implements features

### Scenario 2: Planning + Execution
- Reasoning model creates plan
- Fast model executes steps

### Scenario 3: Review + Fix
- Powerful model reviews code
- Specialized model fixes issues

## 3. Available Models

### How to List Models
```python
list_models()
```

### Model Categories
- **Reasoning models**: mistralai/magistral, o1-preview
- **Coding models**: qwen/qwen3-coder, deepseek-coder
- **General models**: llama3, mixtral
- **Fast models**: phi-3, gemma

## 4. Practical Examples

[10+ detailed examples]

## 5. Best Practices

- Choose model based on task complexity
- Use faster models for iteration
- Use powerful models for final review
- Monitor token usage
- Cache expensive operations

## 6. Performance Considerations

- Model loading time
- Token generation speed
- Memory usage
- Context length

## 7. Troubleshooting

- Model not found
- Model selection errors
- Performance issues
```

**Acceptance Criteria**:
- [ ] Comprehensive coverage
- [ ] 10+ practical examples
- [ ] Best practices clear
- [ ] Troubleshooting complete
- [ ] Easy to navigate

**Review Checkpoint**: Magistral reviews guide completeness

---

#### 3.4 Update TROUBLESHOOTING.md
**Owner**: Claude Code
**Reviewer**: Qwen3-Coder
**Time**: 15 minutes

**File**: `docs/TROUBLESHOOTING.md`

**Add New Section**:

```markdown
## Multi-Model Issues

### Issue: "Model not found"

**Symptoms**:
```
Error: Model 'qwen/qwen3-coder' not found. Available: ['llama3', 'mixtral']
```

**Causes**:
1. Model not loaded in LM Studio
2. Typo in model name
3. Model ID format incorrect

**Solutions**:

1. **Check loaded models**:
   ```python
   list_models()
   ```

2. **Load model in LM Studio**:
   - Open LM Studio
   - Search for model
   - Click "Load"
   - Wait for loading to complete

3. **Use exact model ID**:
   ```python
   # Correct
   model="qwen/qwen3-coder-30b"

   # Wrong
   model="qwen3-coder"  # Missing organization prefix
   ```

### Issue: "Model parameter ignored"

**Cause**: Using old version of lmstudio-bridge-enhanced

**Solution**: Update to v2.0.0+

### Issue: "Wrong model used"

**Symptoms**: Task uses default model instead of specified model

**Solutions**:
1. Check model parameter spelled correctly
2. Verify model loaded in LM Studio
3. Check LM Studio logs for errors
```

**Acceptance Criteria**:
- [ ] Common issues covered
- [ ] Solutions clear
- [ ] Examples provided

**Review Checkpoint**: Qwen3-Coder reviews troubleshooting clarity

---

### Phase 3 Completion Review

**Reviewer**: All 3 LLMs
**Time**: 10 minutes

**Checklist**:
- [ ] API Reference updated
- [ ] README updated
- [ ] Multi-Model Guide complete
- [ ] Troubleshooting updated
- [ ] Examples clear and tested
- [ ] Documentation consistent

**Deliverables**:
- Updated `docs/API_REFERENCE.md`
- Updated `README.md`
- New `docs/MULTI_MODEL_GUIDE.md`
- Updated `docs/TROUBLESHOOTING.md`

---

## Phase 4: Final Testing & Polish (2-2.5 hours)

### Overview
Comprehensive testing, performance validation, and final polish before release.

### Tasks

#### 4.1 End-to-End Testing
**Owner**: Claude Code
**Reviewer**: Qwen3-Coder
**Time**: 1 hour

**Test Scenarios**:

1. **Multi-Model Workflow Test**:
```python
# Load 2 different models in LM Studio first
# Test: Reasoning model â†’ Coding model workflow

# Step 1: Reasoning model explores
result1 = autonomous_with_mcp(
    "filesystem",
    "Analyze project structure and identify missing tests",
    model="mistralai/magistral-small-2509"
)

# Step 2: Coding model generates tests
result2 = autonomous_with_mcp(
    "filesystem",
    f"Based on this analysis: {result1}, generate unit tests",
    model="qwen/qwen3-coder-30b"
)

# Verify both results valid
assert result1 and result2
assert "Error" not in result1
assert "Error" not in result2
```

2. **Model Validation Test**:
```python
# Test: Invalid model handling
result = autonomous_with_mcp(
    "filesystem",
    "Test task",
    model="nonexistent-model"
)

assert "Error" in result
assert "not found" in result.lower()
assert "Available" in result  # Should list available models
```

3. **Backward Compatibility Test**:
```python
# Test: Works without model parameter
result = autonomous_with_mcp(
    "filesystem",
    "List current directory"
    # No model parameter - should use default
)

assert result
assert "Error" not in result
```

4. **Multiple MCPs + Model Test**:
```python
# Test: Model parameter with multiple MCPs
result = autonomous_with_multiple_mcps(
    ["filesystem", "memory"],
    "Analyze code and create knowledge graph",
    model="qwen/qwen3-coder-30b"
)

assert result
```

**Acceptance Criteria**:
- [ ] All scenarios pass
- [ ] Error messages helpful
- [ ] Backward compatibility verified
- [ ] Performance acceptable

**Review Checkpoint**: Qwen3-Coder reviews test results

---

#### 4.2 Performance Benchmarking
**Owner**: Qwen3-Thinking (design) â†’ Claude Code (implementation)
**Reviewer**: Magistral
**Time**: 45 minutes

**File**: `tests/benchmark_multi_model.py` (NEW)

**Benchmarks to Measure**:

1. Model validation overhead
2. Model switching cost
3. Different model performance comparison

```python
"""Benchmark multi-model support performance."""

import asyncio
import time
from tools.dynamic_autonomous import DynamicAutonomousAgent
from llm.model_validator import ModelValidator

async def benchmark_validation_overhead():
    """Measure model validation overhead."""
    validator = ModelValidator()

    # Warm up cache
    await validator.get_available_models()

    # Benchmark cached validation
    start = time.perf_counter()
    for _ in range(100):
        await validator.validate_model("qwen/qwen3-coder-30b")
    end = time.perf_counter()

    avg_time = (end - start) / 100 * 1000  # Convert to ms
    print(f"Validation overhead (cached): {avg_time:.4f} ms")

    # Should be < 0.1ms with cache
    assert avg_time < 0.1

async def benchmark_model_comparison():
    """Compare different models on same task."""
    agent = DynamicAutonomousAgent()

    models = await agent.model_validator.get_available_models()
    if len(models) < 2:
        print("Skipping: Need 2+ models loaded")
        return

    task = "Count to 10"

    results = {}
    for model in models[:2]:  # Test first 2 models
        start = time.perf_counter()
        result = await agent.autonomous_with_mcp(
            "filesystem",
            task,
            max_rounds=10,
            model=model
        )
        end = time.perf_counter()

        results[model] = {
            "time": end - start,
            "result": result
        }
        print(f"\nModel: {model}")
        print(f"Time: {results[model]['time']:.2f}s")

    return results

if __name__ == "__main__":
    asyncio.run(benchmark_validation_overhead())
    asyncio.run(benchmark_model_comparison())
```

**Acceptance Criteria**:
- [ ] Validation overhead < 0.1ms
- [ ] Benchmarks run successfully
- [ ] Results documented

**Review Checkpoint**: Magistral reviews performance results

---

#### 4.3 Documentation Review
**Owner**: All 3 LLMs (collaborative)
**Time**: 30 minutes

**Review Checklist**:

1. **Qwen3-Coder** reviews:
   - [ ] Code examples work
   - [ ] Syntax correct
   - [ ] Examples cover common use cases

2. **Qwen3-Thinking** reviews:
   - [ ] Logical flow
   - [ ] Completeness
   - [ ] Ease of understanding

3. **Magistral** reviews:
   - [ ] Architecture decisions explained
   - [ ] Best practices sound
   - [ ] Troubleshooting comprehensive

**Deliverable**: Consolidated feedback document

---

#### 4.4 Final Polish
**Owner**: Claude Code
**Reviewer**: Qwen3-Coder
**Time**: 15 minutes

**Tasks**:
1. Address documentation feedback
2. Fix any typos or inconsistencies
3. Ensure all examples tested
4. Update version numbers
5. Prepare CHANGELOG entry

**Acceptance Criteria**:
- [ ] All feedback addressed
- [ ] No broken links
- [ ] Examples verified
- [ ] CHANGELOG.md updated

**Review Checkpoint**: Final sign-off from all LLMs

---

### Phase 4 Completion Review

**Reviewer**: All 3 LLMs
**Time**: 15 minutes

**Final Checklist**:
- [ ] End-to-end tests pass
- [ ] Performance benchmarks acceptable
- [ ] Documentation reviewed and polished
- [ ] No regressions introduced
- [ ] Ready for release

**Deliverables**:
- All tests passing
- Performance benchmarks
- Polished documentation
- CHANGELOG.md entry

---

## Task Assignment Matrix

| Phase | Task | Owner | Reviewer | Duration |
|-------|------|-------|----------|----------|
| **Phase 1: Validation Layer** | | | | **2-2.5h** |
| 1.1 | Exception Hierarchy | Claude Code | Qwen3-Coder | 30m |
| 1.2 | Error Handling Utils | Claude Code | Qwen3-Thinking | 45m |
| 1.3 | Model Validator | Qwen3-Coder â†’ Claude | Magistral | 45m |
| 1.4 | Validation Tests | Claude Code | Qwen3-Thinking | 30m |
| **Phase 2: Core Interface** | | | | **2.5-3h** |
| 2.1 | Update Agent Class | Claude Code | Qwen3-Coder | 1h |
| 2.2 | Update Registration | Claude Code | Qwen3-Thinking | 45m |
| 2.3 | Update LLMClient | Claude Code | Qwen3-Coder | 30m |
| 2.4 | Integration Tests | Claude Code | Magistral | 45m |
| **Phase 3: Documentation** | | | | **1.5-2h** |
| 3.1 | API Reference | Claude Code | Qwen3-Thinking | 30m |
| 3.2 | README | Claude Code | Qwen3-Coder | 30m |
| 3.3 | Multi-Model Guide | Qwen3-Thinking â†’ Claude | Magistral | 45m |
| 3.4 | Troubleshooting | Claude Code | Qwen3-Coder | 15m |
| **Phase 4: Testing & Polish** | | | | **2-2.5h** |
| 4.1 | E2E Testing | Claude Code | Qwen3-Coder | 1h |
| 4.2 | Benchmarking | Qwen3-Thinking â†’ Claude | Magistral | 45m |
| 4.3 | Doc Review | All 3 LLMs | - | 30m |
| 4.4 | Final Polish | Claude Code | Qwen3-Coder | 15m |

**Total Duration**: 8-10 hours

---

## Review Cycle Framework

### Review Types

1. **Design Review** (before implementation)
   - Reviewer: Gets design/plan from implementer
   - Validates: Architecture, patterns, edge cases
   - Duration: 10-15 minutes
   - Output: Approval or feedback

2. **Implementation Review** (after coding)
   - Reviewer: Gets code from implementer
   - Validates: Correctness, quality, tests
   - Duration: 15-20 minutes
   - Output: Approval or change requests

3. **Collaborative Review** (for complex tasks)
   - Multiple LLMs review together
   - Validates: Completeness, consistency
   - Duration: 20-30 minutes
   - Output: Consensus approval

### Review Checkpoints

**After Each Task**:
- Implementer notifies reviewer
- Reviewer examines deliverable
- Reviewer provides feedback (approve/request changes)
- Implementer addresses feedback if needed

**After Each Phase**:
- All LLMs review phase deliverables together
- Validate phase completion criteria met
- Sign off before moving to next phase

**Final Review**:
- All LLMs review complete implementation
- Validate all requirements met
- Final sign-off for release

---

## âš ï¸ IMPORTANT: Phase 5 Required for Production

**Status**: ðŸ”´ CRITICAL

After completing Phases 1-4, code reviews from 3 LLMs (Magistral, Qwen3-Coder-30B, Qwen3-Thinking) identified **4 critical production gaps** that must be addressed before production deployment.

**LLM Review Results**:
- Average Rating: **8.0/10**
- Production Readiness: **80%**
- Consensus: "Solid foundation, but critical gaps in streaming, concurrency, and edge cases"

### Critical Gaps Identified

1. **No Streaming Support** ðŸ”´ - 83% of production systems use streaming
2. **Mid-Request Model Switching** ðŸ”´ - Not handled, could cause data corruption
3. **Concurrent Request Safety** ðŸŸ  - No async/await, potential race conditions
4. **Cache Expiration** ðŸŸ  - No TTL, could cause memory bloat

### Phase 5 Timeline

- **Critical Path**: 44 hours (~1 week) - Streaming + Cancellation
- **Recommended Path**: 70 hours (~2 weeks) - Critical + High Priority fixes

**Full Plan**: See `PHASE5_PRODUCTION_HARDENING_PLAN.md` for detailed implementation plan, code examples, and acceptance criteria.

**Target After Phase 5**:
- Rating: 9-10/10
- Production Readiness: 95%+

---

## Success Criteria

### Technical Requirements âœ…
- [ ] Model parameter added to all 3 autonomous tools
- [ ] Model validation implemented
- [ ] Error handling robust
- [ ] Backward compatible (model=None works)
- [ ] All tests pass (>90% coverage)
- [ ] Performance acceptable (<0.1ms validation overhead)

### Documentation Requirements âœ…
- [ ] API Reference updated
- [ ] README updated
- [ ] Multi-Model Guide created
- [ ] Troubleshooting updated
- [ ] Examples tested and working

### Quality Requirements âœ…
- [ ] Code reviewed by 3 LLMs
- [ ] No regressions introduced
- [ ] Consistent with existing codebase
- [ ] Production-ready quality

### Timeline Requirements âœ…
- [ ] Total time: 8-10 hours
- [ ] All phases completed
- [ ] Review cycles included

---

## Deliverables Summary

### New Files (7)
1. `llm/exceptions.py` - Exception hierarchy
2. `utils/error_handling.py` - Retry/fallback utilities
3. `llm/model_validator.py` - Model validation
4. `tests/test_exceptions.py` - Exception tests
5. `tests/test_error_handling.py` - Error handling tests
6. `tests/test_model_validator.py` - Validator tests
7. `tests/test_multi_model_integration.py` - Integration tests
8. `tests/benchmark_multi_model.py` - Performance benchmarks
9. `docs/MULTI_MODEL_GUIDE.md` - Usage guide

### Modified Files (5)
1. `tools/dynamic_autonomous.py` - Add model parameter
2. `tools/dynamic_autonomous_register.py` - Expose model in tools
3. `llm/llm_client.py` - Add error handling
4. `docs/API_REFERENCE.md` - Document model parameter
5. `README.md` - Add multi-model section
6. `docs/TROUBLESHOOTING.md` - Add multi-model issues

### Total: 14 files (9 new + 5 modified)

---

## Risk Mitigation

### Risk 1: Model Validation Overhead
**Mitigation**: 60-second cache for model list
**Validation**: Benchmark shows <0.1ms overhead

### Risk 2: Breaking Changes
**Mitigation**: Optional parameter, backward compatible
**Validation**: Existing tests still pass

### Risk 3: Error Handling Complexity
**Mitigation**: Simple exception hierarchy, clear messages
**Validation**: Comprehensive tests

### Risk 4: Timeline Overrun
**Mitigation**: Break into small tasks, frequent reviews
**Validation**: Track time per task

---

## Post-Implementation

### After Completion
1. Run full test suite
2. Update CHANGELOG.md
3. Tag release (v2.0.0)
4. Announce feature to users
5. Monitor for issues

### Future Enhancements (Out of Scope)
- Automatic model selection based on task
- Model performance analytics
- Concurrent multi-model execution
- Model fallback strategies

---

**Plan Version**: 1.0
**Created**: October 30, 2025
**Authors**: Claude Code + Qwen3-Coder + Qwen3-Thinking + Magistral
**Status**: Ready for Implementation
**Estimated Duration**: 8-10 hours
**Target Release**: v2.0.0
