#!/usr/bin/env python3
"""
Completion tools for LM Studio (chat and text completions).
"""

from typing import Optional, List
from llm.llm_client import LLMClient
import json


class CompletionTools:
    """Tools for generating completions from local LLMs."""

    def __init__(self, llm_client: Optional[LLMClient] = None):
        """Initialize completion tools.

        Args:
            llm_client: Optional LLM client (creates default if None)
        """
        self.llm = llm_client or LLMClient()

    async def chat_completion(
        self,
        prompt: str,
        system_prompt: str = "",
        temperature: float = 0.7,
        max_tokens: int = 1024
    ) -> str:
        """Generate a completion from the current LM Studio model.

        Args:
            prompt: The user's prompt to send to the model
            system_prompt: Optional system instructions for the model
            temperature: Controls randomness (0.0 to 1.0)
            max_tokens: Maximum number of tokens to generate

        Returns:
            The model's response to the prompt
        """
        try:
            messages = []

            # Add system message if provided
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})

            # Add user message
            messages.append({"role": "user", "content": prompt})

            response = self.llm.chat_completion(
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens
            )

            # Extract the assistant's message
            choices = response.get("choices", [])
            if not choices:
                return "Error: No response generated"

            message = choices[0].get("message", {})
            content = message.get("content", "")

            if not content:
                return "Error: Empty response from model"

            return content
        except Exception as e:
            return f"Error generating completion: {str(e)}"

    async def text_completion(
        self,
        prompt: str,
        temperature: float = 0.7,
        max_tokens: int = 1024,
        stop_sequences: Optional[List[str]] = None
    ) -> str:
        """Generate a raw text completion (non-chat format) from LM Studio.

        This endpoint is simpler and faster than chat completions for single-turn tasks
        like code completion, text continuation, or simple Q&A.

        Args:
            prompt: The text prompt to complete
            temperature: Controls randomness (0.0 to 2.0, default 0.7)
            max_tokens: Maximum number of tokens to generate (default 1024)
            stop_sequences: Optional list of sequences where generation will stop

        Returns:
            The completed text from the model
        """
        try:
            response = self.llm.text_completion(
                prompt=prompt,
                temperature=temperature,
                max_tokens=max_tokens,
                stop_sequences=stop_sequences
            )

            # Extract the completion text
            choices = response.get("choices", [])
            if not choices:
                return "Error: No completion generated"

            text = choices[0].get("text", "")

            if not text:
                return "Error: Empty completion from model"

            return text
        except Exception as e:
            return f"Error generating text completion: {str(e)}"

    async def create_response(
        self,
        input_text: str,
        previous_response_id: Optional[str] = None,
        stream: bool = False,
        model: Optional[str] = None
    ) -> str:
        """Create a response using LM Studio's stateful /v1/responses endpoint.

        This endpoint provides stateful conversations where you can reference previous
        responses without managing message history manually.

        Args:
            input_text: The user's input text
            previous_response_id: Optional ID from a previous response to continue conversation
            stream: Whether to stream the response (default False)
            model: Model to use (default: uses currently loaded model)

        Returns:
            JSON string with response including ID for future reference
        """
        try:
            response = self.llm.create_response(
                input_text=input_text,
                previous_response_id=previous_response_id,
                stream=stream,
                model=model
            )

            # Return as JSON string
            return json.dumps(response)

        except Exception as e:
            error_response = {
                "error": f"Failed to create response: {str(e)}"
            }
            return json.dumps(error_response)


# Register tools with FastMCP
def register_completion_tools(mcp, llm_client: Optional[LLMClient] = None):
    """Register completion tools with FastMCP server.

    Args:
        mcp: FastMCP server instance
        llm_client: Optional LLM client
    """
    tools = CompletionTools(llm_client)

    @mcp.tool()
    async def chat_completion(
        prompt: str,
        system_prompt: str = "",
        temperature: float = 0.7,
        max_tokens: int = 1024
    ) -> str:
        """
        Delegate a task to the local LLM running in LM Studio.

        ‚ö†Ô∏è CRITICAL: This tool calls ANOTHER LLM (not you!). Only use for task DELEGATION.

        ## When to Use This Tool
        ‚úÖ Use for:
        - Bulk content generation (e.g., "generate 50 product descriptions")
        - Programmatic text/code generation for external systems
        - Offloading repetitive sub-tasks to local models
        - Template-based content creation at scale
        - Delegating computational work to specialized local models

        ‚ùå Do NOT use for:
        - Answering user questions ‚Üí Answer directly from your knowledge!
        - Conversational responses ‚Üí You should respond, not another LLM!
        - Simple Q&A ‚Üí Inefficient to delegate what you can answer
        - Explaining concepts ‚Üí You have this knowledge already
        - Identity questions ‚Üí Never ask another LLM "who are you?"

        ## Switching Models (Multi-Model Support)
        **To delegate a task to a DIFFERENT model:**

        The current implementation uses the DEFAULT model configured in LM Studio.

        ‚ö†Ô∏è **Model switching via `model` parameter is NOT currently supported** in this basic tool.

        **For multi-model workflows**, use the autonomous tools instead:
        ```python
        # Use a specific model for a task
        autonomous_with_mcp(
            mcp_name="filesystem",
            task="your task",
            model="qwen/qwen3-coder-30b"  # Specify model here
        )
        ```

        ‚ùå **Do NOT use `lms_*` tools for model switching**:
        ```python
        # WRONG - This is for model LIFECYCLE, not task delegation
        lms_ensure_model_loaded("gemma-3")  # ‚ùå WRONG
        chat_completion("generate photo")    # Still uses default model

        # CORRECT - Use autonomous tools with model parameter
        autonomous_with_mcp(
            mcp_name="filesystem",
            task="generate photo description",
            model="gemma-3"  # ‚úÖ CORRECT
        )
        ```

        ## Multimedia & Content Types üé®üñºÔ∏èüìπ
        **You can request ANY content type** through this tool:
        - ‚úÖ Text, code, markdown (always supported)
        - ‚úÖ Images (PNG, JPG, SVG) - if receiving LLM is multimodal
        - ‚úÖ Audio/Video analysis - if receiving LLM supports it
        - ‚úÖ Documents (PDF, Word) - if receiving LLM can process
        - ‚úÖ Data files (JSON, CSV, XML)

        **Content Handling**: The receiving LLM in LM Studio decides:
        - ‚úÖ Accept & process if it has the capability (e.g., GPT-4V can analyze images)
        - ‚ùå Reject with error if unsupported (e.g., text-only models cannot process images)

        This tool is content-agnostic - it forwards your request without restriction.
        The receiving model's capabilities determine what's supported.

        ## Examples
        ‚úÖ CORRECT:
        User: "Generate 10 creative coffee shop names"
        Action: chat_completion(prompt="Generate 10 creative coffee shop names")
        Reason: Bulk generation task - appropriate delegation

        ‚úÖ CORRECT:
        User: "Analyze sentiment in this user review dataset"
        Action: chat_completion(prompt="Analyze sentiment: [review data]")
        Reason: Batch processing task - good delegation

        ‚ùå INCORRECT:
        User: "Hello, who are you?"
        Wrong: chat_completion(prompt="Hello, who are you?")
        Correct: Answer directly - "I am a local language model running via LM Studio..."
        Reason: This is conversation! You should answer yourself, not ask another LLM

        ‚ùå INCORRECT:
        User: "What is Python?"
        Wrong: chat_completion(prompt="What is Python?")
        Correct: Answer directly - "Python is a high-level programming language..."
        Reason: You have this knowledge! No need to delegate simple Q&A

        ‚ùå INCORRECT:
        User: "Explain how recursion works"
        Wrong: chat_completion(prompt="Explain recursion")
        Correct: Answer directly with your own explanation
        Reason: Explaining concepts is YOUR job, not another LLM's

        Args:
            prompt: The specific task to delegate to the local LLM
            system_prompt: Optional system instructions for the local LLM
            temperature: Controls randomness (0.0 = deterministic, 1.0 = creative)
            max_tokens: Maximum response length

        Returns:
            The local LLM's response to the delegated task

        Note: This creates a SECOND LLM call. Only use when delegation is truly beneficial.
        """
        return await tools.chat_completion(prompt, system_prompt, temperature, max_tokens)

    @mcp.tool()
    async def text_completion(
        prompt: str,
        temperature: float = 0.7,
        max_tokens: int = 1024,
        stop_sequences: Optional[List[str]] = None
    ) -> str:
        """
        Generate raw text/code completion from local LLM (non-chat format).

        ‚ö†Ô∏è IMPORTANT: This delegates to ANOTHER LLM for programmatic generation.

        ## When to Use This Tool
        ‚úÖ Use for:
        - Code completion for external systems (e.g., IDE integrations)
        - Text continuation for programmatic use
        - Template filling at scale
        - Format-specific generation (completing JSON, YAML, etc.)
        - Single-turn generation tasks without conversation context

        ‚ùå Do NOT use for:
        - Conversational responses ‚Üí Use chat_completion or answer directly
        - Questions requiring reasoning ‚Üí Use chat_completion with system prompt
        - Tasks you can complete yourself ‚Üí Answer directly!
        - Multi-turn conversations ‚Üí Use chat_completion instead

        ## Multimedia & Content Types üé®üñºÔ∏èüìπ
        **You can request ANY content type** through this tool:
        - ‚úÖ Text, code, markdown (primary use case)
        - ‚úÖ SVG/ASCII art (text-based visuals)
        - ‚úÖ Data formats (JSON, CSV, XML)
        - ‚úÖ Configuration files (YAML, TOML, INI)

        **Content Handling**: The receiving LLM in LM Studio decides:
        - ‚úÖ Accept & process if it can generate the requested format
        - ‚ùå Reject with error if unsupported

        This tool is optimized for single-turn, format-specific generation.

        ## Examples
        ‚úÖ CORRECT:
        User: "Complete this function signature"
        Action: text_completion(prompt="def fibonacci(n):")
        Reason: Code completion for external system

        ‚úÖ CORRECT:
        User: "Fill in this JSON template"
        Action: text_completion(prompt='{"name": "', stop_sequences=['"'])
        Reason: Format-specific generation with stop sequence

        ‚ùå INCORRECT:
        User: "What does this code do?"
        Wrong: text_completion(prompt="What does this code do: ...")
        Correct: Answer directly or use chat_completion with context
        Reason: This is analysis/explanation, not text completion

        ‚ùå INCORRECT:
        User: "Help me debug this"
        Wrong: text_completion(prompt="Debug this code...")
        Correct: Use chat_completion or reason about it yourself
        Reason: Debugging requires conversation context, not raw completion

        Args:
            prompt: The text prompt to complete (programmatic generation)
            temperature: Controls randomness (0.0 = deterministic, 2.0 = creative)
            max_tokens: Maximum number of tokens to generate
            stop_sequences: Optional list of sequences where generation stops

        Returns:
            The completed text from the local LLM

        Note: Faster than chat_completion but no conversation context.
        """
        return await tools.text_completion(prompt, temperature, max_tokens, stop_sequences)

    @mcp.tool()
    async def create_response(
        input_text: str,
        previous_response_id: Optional[str] = None,
        stream: bool = False,
        model: Optional[str] = None
    ) -> str:
        """
        Create stateful conversation response with local LLM (maintains history automatically).

        ‚ö†Ô∏è NOTE: This delegates to ANOTHER LLM with automatic conversation state management.

        ## When to Use This Tool
        ‚úÖ Use for:
        - Multi-turn conversations with the local LLM
        - Maintaining conversation context automatically
        - Continuing previous conversation threads
        - Delegated conversational tasks requiring memory

        ‚ùå Do NOT use for:
        - Single-turn questions ‚Üí Use chat_completion or answer directly
        - Your own conversational responses ‚Üí Answer directly!
        - Simple Q&A ‚Üí Inefficient delegation
        - Bulk generation ‚Üí Use chat_completion instead

        ## Multimedia & Content Types üé®üñºÔ∏èüìπ
        **You can request ANY content type** through this tool:
        - ‚úÖ Text, code, markdown (always supported)
        - ‚úÖ Images (if receiving LLM is multimodal)
        - ‚úÖ Audio/Video (if receiving LLM supports it)

        **Content Handling**: The receiving LLM in LM Studio decides:
        - ‚úÖ Accept & process if it has the capability
        - ‚ùå Reject with error if unsupported

        ## Examples
        ‚úÖ CORRECT:
        Turn 1: create_response("Explain Python")
        Turn 2: create_response("Give me an example", previous_response_id=resp1_id)
        Reason: Multi-turn conversation delegation with context

        ‚ùå INCORRECT:
        User: "Hello, how are you?"
        Wrong: create_response("Hello, how are you?")
        Correct: Answer directly - "I'm an AI assistant, ready to help!"
        Reason: This is YOUR conversation, not a delegated task

        Args:
            input_text: The input for the local LLM
            previous_response_id: ID from previous response (for conversation continuity)
            stream: Whether to stream the response
            model: Model to use (default: currently loaded model)

        Returns:
            JSON string with response and ID for future reference

        Note: Server maintains conversation history automatically.
        """
        return await tools.create_response(
            input_text, previous_response_id, stream, model
        )


__all__ = [
    "CompletionTools",
    "register_completion_tools"
]
